{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a brief summary of the steps I have taken in performing data wrangling as it pertains to the WeRateDogs Twitter archive data project as part of the Udacity Data Analyst Nanodegree program.\n",
    "\n",
    "Data Wrangling consists of the following steps:\n",
    "- Gather data\n",
    "- Assess data\n",
    "- Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three pieces of data that I needed to gather.\n",
    "1. WeRateDogs Twitter archive.  It was given for the project, therefore it was manually downloaded with a link.\n",
    "\n",
    "2. Tweet image predictions.  This file is hosted on Udacity's server and was downloaded programmatically using the *Requests* library and the given URL.\n",
    "\n",
    "3. Each tweet's retweet count and favorite count.  This was obtained by querying the Twitter API for each tweet's JSON data using Python's Tweepy library\n",
    "\n",
    "The process of obtaining data from the Twitter API takes about 30 minutes of runtime because there's a rate limit of 900 requests for every 15 minutes that is imposed. Each API request is essentially the tweet id in the Twitter archive file, with a total of 2356.  Each request returns a tweet object that contains various tweet related attributes.  In my approach, I only grabbed what I needed (favorite and retweet count) after each API request and append to the dictionary where we have the tweet id, favorite, retweeted, and each of their respective numerical values.  There are instances where the API request results in an error.  These errors are appended to an error list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to assess the data both visually and programatically for quality and tidiness issues.  Below is a summary of issues found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality\n",
    "##### `tweet_api` table\n",
    "- *tweet_id* is int64 and should be a string since it's not being used for numerical calculations but rather to as an id\n",
    "\n",
    "##### `twitter_archive` table\n",
    "- *tweet_id* is int64 and should be a string since it's not being used for numerical calculations but rather to as an id\n",
    "- *rating_numerator* has unrealistically large values\n",
    "- *rating_denominator* are not all 10\n",
    "- *timestamp* is a string and not datetime, and remove +0000 from the end\n",
    "- *retweeted_status_timestamp*, *retweeted_status_id*, *retweeted_status_user_id*, *in_reply_to_status_id*, *in_reply_to_status_id* columns can be removed since they are associated with retweets, and not original tweets\n",
    "- *name* that are in lowercase is incorrect eg. shows \"a\" but should be \"none\" since the name was never mentioned in the tweet text\n",
    "- \"None\" under each of the 4 dog stage columns should be replaced with NaN\n",
    "- *source* column contains unnecessary text.  Clean up data such that it only shows iphone, twitter web, vine, or tweet deck.\n",
    "- Gender of the dog in the tweet can be extracted from the *text* column by trying to match certain keywords such as he, she, him, her, etc. It wil be either male, female, or NaN (not specified), add a column *gender*.  (not a quality issue but more like additional info that can be extracted from existing data)\n",
    "- Convert data in the *stage* column from string to categorical type (after converting 4 columns into 1)\n",
    "- 11 of the rows have 2 stages and need to be manually cleaned.\n",
    "\n",
    "##### `image_predictions` table\n",
    "- *p1*,*p2* and *p3* have mix of uppercase and lowercase characters for the dog breed\n",
    "\n",
    "### Tidiness\n",
    "\n",
    "##### `twitter archive` table\n",
    "- dog stage should be in one column instead of four different ones (puppo, pupper, doggo, floofer) so that it satisfies rules of tidy data: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell.\n",
    "\n",
    "\n",
    "\n",
    "##### Overall\n",
    "- Merge *tweet_api*, *twitter_archive*, and *image_predictions* into one table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A copy of each of the 3 dataframes were made prior to the cleaning process.  All the issues identified in the assess data were cleaned programmatically using pandas and tested to ensure that it worked.  At the end I used the pandas merge function to merge the 3 dataframes into one and exported to twitter_archive_master csv file to use in the data analysis and visualization section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
